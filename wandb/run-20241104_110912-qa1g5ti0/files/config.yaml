_wandb:
    value:
        cli_version: 0.18.5
        m:
            - "1": trainer/global_step
              "6":
                - 3
              "7": []
            - "1": val_SDA_squared_error
              "5": 1
              "6":
                - 1
                - 3
              "7": []
            - "1": epoch
              "5": 1
              "6":
                - 1
                - 3
              "7": []
            - "1": train_SDA_squared_error
              "5": 1
              "6":
                - 1
                - 3
              "7": []
            - "1": test_SDA_squared_error
              "5": 1
              "6":
                - 1
                - 3
              "7": []
        python_version: 3.10.12
        t:
            "1":
                - 1
                - 9
                - 55
                - 103
            "2":
                - 1
                - 9
                - 55
                - 103
            "3":
                - 7
                - 13
                - 23
                - 55
                - 66
            "4": 3.10.12
            "5": 0.18.5
            "8":
                - 5
            "12": 0.18.5
            "13": linux-x86_64
config:
    value:
        BATCH_SIZE: 128
        GRAD_ACCUMULATION_STEPS: 2
        MAX_EPOCHS: 2
        SDA_params:
            hidden_dims:
                - 800
                - 400
                - 100
            input_dim: 1280
            mask_fraction: 0.2
        callbacks:
            EarlyStopping:
                min_delta: 5e-05
                mode: min
                monitor: val_SDA_squared_error
                patience: 8
            ModelCheckpoint:
                mode: min
                monitor: val_SDA_squared_error
                save_last: true
                save_top_k: 1
        ckpt_file_name: '{epoch}-{val_SDA_squared_error:.2f}'
        loss_fn: SDA_squared_error
        lr: 0.001
        num_workers: 8
        save_dir: results
        weight_decay: 0.0001
